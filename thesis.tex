\documentclass[msc,oneside]{ubcthesis}%msc, phd, masc, ma, or meng

% ================================================================================
% CHANGE THE FOLLOWING ACCORDING TO YOUR PROGRAM/THESIS
% ================================================================================
\institution{The University Of British Columbia}
\faculty{Unit 5}
\institutionaddress{Okanagan}

% For an Honours thesis, use \documentclasss[msc,oneside]{ubcthesis} above and
% uncomment and modify the next line:
\degreetitle{B.Sc. Computer Science Honours}

\title{\emph{Rally}, a One Stop-Shop for \textit{Reddit} data and Insights}
\author{Kevin J. Eger}
\copyrightyear{2016}
\submitdate{April 2016} % date of approved thesis
\program{Computer Science}%or Mathematics, or Interdisciplinary Studies

% ================================================================================


\usepackage{ubcostyle} %loads packages


% ===================================================================
% CHANGE THE FOLLOWING COMMANDS ACCORDING TO YOUR NEEDS
% ===================================================================
\newcommand{\R}{\mathbb{R}}   %real number
\newcommand{\Z}{\mathbb{Z}}   %integers
\newcommand{\C}{\mathbb{C}}   %complex numbers

\newcommand{\dom}{\operatorname{dom}}
\providecommand{\TT}[1]{\Theta\left(#1\right)} % big-Theta
\providecommand{\OO}[1]{\mathcal{O}\left(#1\right)} % big-Oh
\setlength{\parskip}{1em}
% ===================================================================

%Uncomment the next line if there are more than one appendix
%\renewcommand*\appendixname{Appendices}

\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage[section]{placeins}

\usepackage{xcolor}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{xparse}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{csvsimple}
\usepackage{etoolbox}
\usepackage{multicol}
\usepackage[htt]{hyphenat}
\newenvironment{twocolitemize}{%
\begin{itemize}
\begin{multicols}{2}
}{%
\end{multicols}
\end{itemize}
}    

\lstset{	
	language=PHP,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}},
    tabsize=2
}

\lstdefinelanguage{PHP}{
	commentstyle = \color{gray},
    extendedchars = \true,
    inputencoding = utf8x,
    keepspaces = true,
    keywordstyle = \bfseries,
    morekeywords={function,return},
}

\NewDocumentCommand{\sqlword}{v}{%
\texttt{\textcolor{gray}{#1}}%
}

\NewDocumentCommand{\inlinemathword}{v}{%
\texttt{#1}%
}

\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

\begin{document}

% This starts numbering in Roman numerals as required for the thesis
% style.
\frontmatter                    % Mandatory

% The order of the following components should be preserved.  The order
% listed here is the order currently required by FoGS.
\maketitle                      % Mandatory

\begin{abstract}                % Mandatory -  maximum 350 words
\textit{\textit{Reddit}} is \emph{the front page of the internet}, a slogan the company has coined and rightfully lived up to. It is a website which brings together members of all communities in a similar style to a typical forum but with much more structure and a lot more traffic. The open nature of \textit{Reddit} captures over 200 million unique visitors a month. With such traffic screams the demand for data analysis through a human-interpretable medium. Data analysis on \textit{Reddit} has been done before, but this thesis focuses on bringing the data gathered in to an easily consumable format. We will explore the implementation and results of querying the \textit{Reddit} API, generating aggregate statistics, querying large data dumps of historic \textit{Reddit} data with \emph{Google BigQuery}, analyzing and labelling the content of \textit{Reddit} using \textit{Google Cloud Vision}'s image recognition, providing an innovative technique for consuming \textit{Reddit} and the use of unsupervised machine learning to draw powerful conclusions. The result is a system called \textit{Rally} which brings together the busy and wild community of \textit{Reddit} through clear and effective data aggregation, inference and visualization.
\end{abstract}

\newpage
\phantomsection \label{tableofcontent}%set anchor at right location
\addcontentsline{toc}{chapter}{\contentsname}
\tableofcontents                % Mandatory: generate toc
\newpage 
\phantomsection \label{listoftab}%set anchor at right location
\addcontentsline{toc}{chapter}{\listtablename}
\listoftables                   % Mandatory if thesis has tables
\newpage
\phantomsection \label{listoffig}%set anchor at right location
\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures                  % Mandatory if thesis has figures


\chapter{Acknowledgements}      % Optional
Work on this thesis was widely facilitated with help from my supervisor, Dr. Ramon Lawrence, through weekly meetings where ideas and progress were discussed extensively. It is also important to acknowledge Dr. Jeff Andrews for his support in advising on machine learning techniques which were implemented as described later. I would also like to acknowledge the \textit{Reddit} community as a whole for their feedback on a portion of this project which was launched for public use.



% Any other unusual prefactory material should come here before the
% main body.

% Now regular page numbering begins.
\mainmatter\setlength{\parskip}{1em}

% Parts are the largest structural units, but are optional.
%\part{Thesis}

% Chapters are the next main unit.
\chapter{Introduction}
\textit{Reddit} is a news and entertainment website whose content is generated by members of the community. Users submit text posts or direct links similar to a typical forum setting. Registered users can vote on submissions, yielding an ordered online bulletin board. Furthermore, what makes \textit{Reddit} unique is that content is subsectioned into different areas of interest called ``subreddits''. Some of the top subreddits include \textit{movies}, \textit{funny}, \textit{AskReddit}, \textit{food} and \textit{news}. As of March 3rd, 2016 \textit{Reddit} had 231,625,384 unique users a month viewing a total of 7,517,661,034 pages. The company was founded 10 years ago and has quickly become the most central place on the internet to partake in conversation or consume a wide array of content.

For years, data analysis has been used in many industries to give companies and organizations more information to make business decisions and verification of their models and structures. Whether they are mining huge data sets, looking at specific use cases or aiming to prove or disprove a theory, companies and organizations alike aim to do one thing: identify and discover patterns, relationships and inferences that are not immediately apparent.
\par
A motivator for this thesis was existing technology for \textit{Twitter} insights. The community-content driven nature of \textit{Twitter} parallels that of \textit{Reddit}. There has already been a lot of academic research and production level software released for \textit{Twitter} data management, pattern identification and tracking and the existing infrastructure in the \textit{Twitter} space can be largely replicated and modified to suit \textit{Reddit}.

\textit{Rally} is the name of the implemented suite of tools built. The inspiration for the name is derived from the formal definition of the word: ``to come together''. \textit{Rally} combines the accessing, processing, aggregating and visualising of \textit{Reddit} data in one central implementation. It is delivered in the form of a web application so as to be accessible by the largest array of people possible. This thesis will cover the research, experimentation and considerations that ultimately produced the final ``product'', \textit{Rally}.

\chapter{Background}
To best understand this thesis and the work done, it is necessary to be introduced to the relevant technologies and key terms which will be heavily referenced and built upon.

\section{Key Terms and Definitions}
\begin{itemize}
\item{\textit{Reddit}: entertainment, social news network service (website)}
\item{Karma: how much good a user has done for the \textit{Reddit} community quantified by submissions links that people upvote}
\item{\textit{Google Cloud Platform}: Google's core infrastructure, data analytics and machine learning}
\item{API: application program interface}
\item{\textit{CUDA}: parallel computer platform for harnessing the power of an Nvidia GPU (graphics processing unit)}
\item{Subreddit: a \textit{niche} forum on \textit{Reddit} focused around a common topic or content type}
\item{AJAX: web development techniques used on the client-side to create asynchronous requests}
\end{itemize}

\section{Reddit}

\subsection{History}
The company was founded by two new graduates of the \textit{University of Virginia}, Steve Huffman and Alexis Ohanian, in June 2005 \citep{Guardian}. After a couple years of growth, \textit{Reddit}'s traffic exploded and the service went viral. The creators were quick to release \textit{Reddit} Gold, which offered new features and usability improvements, providing the company with a primary source of income.

\subsection{Community}
\textit{Reddit} thrives on its open nature and diverse content fully generated by the community \citep{Atlantic}. The demographics \textit{Reddit} serves allows for a wide range of subject areas thus having the ability for smaller communities to digest their niche content. Subreddits provide a very unique opportunity by raising attention and fostering discussion that may not be seen as mainstream and covered by other news or entertainment mediums.
\par{}
\textit{Reddit} as a company and as a community has been known for several philanthropic projects both short and long term. A few notable efforts are:
\begin{itemize}
\item{Users donated \$185,356 to Direct Relief for Haiti after the earthquake that struck the country in January 2010}
\item{\textit{Reddit} donates 10\% of it's yearly annual ad revenue to non-profits voted upon by its users \citep{RedditBlog}}
\item{Members from \textit{Reddit} donated over \$600,000 to DonorsChoose in support of Stephen Colbert's March to Keep Fear Alive \citep{DonorsChoose}}
\end{itemize}

\chapter{Technical Stack}
\textit{Rally} is a project that explores many different types of data accessing methods, processing techniques and visualizations. Due to the nature of web applications, it is no surprise that \textit{Rally} is implemented with modular programming as a key focus. Several design choices and system architecture decisions are what will allow this project to be easily continued and built on. The technical stack is broken in to components as follows.

\section{Laravel}
\textit{Laravel} is a PHP web application framework with expressive, elegant syntax \citep{Laravel}. \textit{Laravel} is designed primarily with the motive of removing the repetitive and trivial tasks associated with the construction of a majority of web projects (ie: authentication, routing, sessions, etc.). \textit{Laravel} aims to make the development process a pleasing one for the developer without sacrificing application functionality \citep{Laravel}. The accessible and powerful framework was chosen for its existing familiarity and power to implement a project spanning many domains.

\subsection{MVC}
\textit{Laravel} follows the traditional Model-View-Controller design pattern. Models interact with the database through the \textit{Eloquent} ORM providing an object oriented handle on information. Controllers handle the requests and retrieving data by leveraging the models. Views render the web pages and are returned to the user.
\par
This intrinsic design pattern was followed tightly alongside the addition of a repository layer. As discussed later, \textit{Rally} interacts with several external resources such as the \textit{Reddit} API and the \textit{Google Cloud Platform}. These external resources house gigabytes of data, thus storing them locally and accessing them through a model is counterproductive. To retain the structure of the MVC framework, a repository layer is built on top of the models. This allows for the convenience of a seemingly object oriented interaction with data outside of the application. Not only does it allow for convenient method calls but also abstracts logic away from the controllers, leaving them as slim as possible. This is a vital design philosophy to modern web development as it modularizes code to ensure a more rigid flow and testable code-base. Basic examples from \textit{Rally} utilizing each level of the MVC framework as well as the repository layer. An example of model, view, controller and repository use are seen in Figure \ref{fig:modelex}, \ref{fig:viewex}, \ref{fig:contex} and \ref{fig:repository} respectively.
\par
% Model Example
\begin{figure}[H]
\begin{center}
\begin{lstlisting}
$cluster_image = Cluster::where("name", $subreddit)->first();
\end{lstlisting}
\end{center}
\caption[Example of Model]{
Example of retrieving the first \textit{Cluster} model where the name field matches.}
\label{fig:modelex}
\end{figure}

% View Example
\begin{figure}[H]
\begin{lstlisting}
<select name="labels" . . . multiple="">
	@foreach($labels as $label)
		<option value="{{ $label }}">{{ $label }}</option>
	@endforeach
</select>
\end{lstlisting}
\caption[Example of View]{
Example demonstrating how objects passed to the view are utilized and iterated over to display the options for the index page of \textit{Content Search}. \textit{Laravel} leverages an HTML templating engine called \textit{Blade} which allows for convenient variable dumping and interaction.}
\label{fig:viewex}
\end{figure}

% Controller Example
\begin{figure}[H]
\begin{lstlisting}
public function show(Request $request)
{
	$subreddit = $request->get("subreddit");
	$about = $this->phpraw->aboutSubreddit($subreddit);
	
	return response()->view("subreddit.show", [
		"subreddit" => $subreddit,
		"about"     => $about->data,
		"tagline"   => "A look at /r/" . $subreddit
	]);
}
\end{lstlisting}
\caption[Example of Controller]{
Example of the show() function in the Subreddit Controller. This method retrieves the necessary data, then sends the data to a blade view (subreddit/show.blade.php) and returns a rendered instance of that view.}
\label{fig:contex}
\end{figure}

% Repository Example
The repository layer is utilized primarily to wrap auxiliary data sources. This gives them a similar feel and interaction as a traditional model. Seen in Figure \ref{fig:repository}, a RedditorRepository instance is injected in to the RedditorsController class which is then used in its internal functions to gather data using the phpRaw \textit{Reddit} API wrapper in a chainable method technique identical to a traditional model.
\begin{figure}[!htb]
\begin{lstlisting}
protected $redditor;

public function __construct(RedditorRepository $redditor)
{
	$this->redditor = $redditor;
}
...
public function show(Request $request)
{
	$user = $request->redditor;
	$subreddits = $this->redditor->getUserSubmitted($user)->getSubredditsList();
	...
}
\end{lstlisting}
\caption[Example of Repository]{
Code snippets from the Redditor Controller which leverages the power of a repository layer to make chain-able function calls to an auxiliary data source.}
\label{fig:repository}
\end{figure}

\section{Storage}
Databases used to house the necessary persistent information for the application. A local \textit{MySQL} database and cloud-based \textit{BigQuery} database.

\subsection{\textit{MySQL}}
\textit{MySQL} is an open-source relational database management system (DBMS). In \textit{Laravel}, it is the default database system largely because of its \textit{plug and play} nature. The \textit{MySQL} database is what saves the caching layer as described in detail throughout the implementation section. A visual representation of the schema can be seen in Figure \ref{fig:erdia}.

\begin{figure}[!htb]
\includegraphics[width=\textwidth]{mysql_er_April_2.png}
\caption[MySQL Database schema]{
An ER diagram representing the \textit{MySQL} database schema.}
\label{fig:erdia}
\end{figure}

\subsection{BigQuery}
Querying massive datasets can not only be time consuming but expensive without the right hardware, infrastructure and software. \textit{Google} alleviates this problem with \textit{BigQuery}, an incredibly fast cloud-based storage platform. It is infrastructure as a service (IaaS) that handles all the hard work of both creating and accessing large data sets. Using the processing power of \textit{Google}, a user can get up and running with \textit{BigQuery} in a matter of minutes. The service can be used via their web UI, command-line tool or the REST API using one of the many client libraries. 
\par
In November 2015, user \texttt{/u/Stuck\_In\_the\_Matrix} of \textit{Reddit} collected all submission data from 2006 to 2015. He had effectively bundled 200 million submission objects, each with score data, author, title, self{\_}text, media tags and all the other attributes that are normally available via the \textit{Reddit} API. The dataset complemented the \textit{Reddit} comment corpus he released a couple months prior. When the data was initially made publicly available, he released it as a torrent where developers interested in using it could download their own local copies. Developers were all downloading the data for use either on their local machines or a cloud server. The problem with this is even with one of the most powerful desktop computers, loading the entire dataset into RAM was not feasible. Search times and joining (cross table) operations were expensive. 
\par
Soon after the release of this torrent, one of the lead engineers of \textit{Google BigQuery}, Felipe Hoffa, uploaded the data to \textit{BigQuery} and made the dataset publicly available. Each month, the dataset is updated with the latest information collected from the \textit{Reddit} API.
\par
With the convenience of \textit{BigQuery}, it is now possible to query gigabytes of historic \textit{Reddit} data in a matter of seconds. Listed below are a couple of the integral queries used in \textit{Rally}, their sizes and execution times.

% Best time to post on \textit{Reddit}
\begin{figure}[H]
\begin{lstlisting}
SELECT subreddit, total, sub_hour, num_gte_3000
FROM (
	SELECT
		HOUR(SEC_TO_TIMESTAMP(created - 60*60*5)) as sub_hour,
		SUM(score >= 3000) as num_gte_3000,
		SUM(num_gte_3000) OVER(PARTITION BY subreddit) total, subreddit,
	FROM [fh-bigquery:Reddit_posts.full_corpus_201509]
	WHERE YEAR(SEC_TO_TIMESTAMP(created))=2015
	GROUP BY sub_hour, subreddit
	ORDER BY subreddit, sub_hour
)
WHERE total>700
ORDER BY total DESC, sub_hour
\end{lstlisting}
\caption[Query finding the best hours to post on \textit{Reddit}]{
The BigQuery SQL for finding the best hours to post on \textit{Reddit}. This query processes 5.00GB across one table in roughly 8 seconds (~1.5 seconds when cached)}
\end{figure}

% Activity on \textit{Reddit} over time
\begin{figure}[H]
\begin{lstlisting}
SELECT RIGHT('0'+STRING(peak),2)+'-'+subreddit, hour, c 
FROM (
  SELECT subreddit, hour, c, MIN(IF(rank=1,hour,null)) 
  OVER(PARTITION BY subreddit) peak 
  FROM (
    SELECT subreddit, HOUR(SEC_TO_TIMESTAMP(created_utc)) hour, COUNT(*) c, ROW_NUMBER() 
    OVER(PARTITION BY subreddit ORDER BY c ) rank 
    FROM [fh-bigquery:Reddit_comments.2015_08] 
    WHERE subreddit IN (%subreddits) 
    AND score>2 
    GROUP BY 1, 2 )
    )
ORDER BY 1,2
\end{lstlisting}
\caption[Query finding the best hours to post on \textit{Reddit}]{
Viewing activity (number of submissions) on subreddits over time. The wildcard \textit{\%subreddits} is replaced with a comma-separated string of subreddits. This query processes 1.49GB across one table in roughly 2.5 seconds (roughly 1.1 seconds when cached).}
\end{figure}

\subsubsection{Facades in Laravel with Google Services}
In web programming, quite often developers will need access to static references of classes. Facades provide a static interface to such classes that are available in the application's service container. By default \textit{Laravel} ships with several facades. These static proxies to underlying classes in the service container provide the benefit of a terse, expressive syntax while maintaining more testability and flexibility than traditional static methods.
\par
The facade class itself only needs to implement a single method \texttt{getFacadeAccessor()}. It is that method's job to define what to resolve from the container. Behind the scenes, the base facade class (which all facades must extend) makes use of a magic-method, \texttt{\_\_callStatic()}, which defers calls from the facade to the resolved object. 

% Registering the service provider
\begin{figure}[H]
\begin{lstlisting}
public function register()
{
	$this->app->bind('google', function () {
		$client = new Google_Client();
		$client->useApplicationDefaultCredentials();
		$client->addScope(Google_Service_Bigquery::BIGQUERY);
		
		return new GoogleAPI($client);
	});
}
\end{lstlisting}
\caption[Registering the Google service provider]{
Registering the \textit{Google} service provider and binding the facade keyword \textit{Google} to it.}
\end{figure}

The point of registering a facade may at times seem convoluted and unnecessary. It has always been a topic of discussion amongst the PHP world and a lot of the time boils down to personal preference and code readability. The facade approach was chosen particularly for \textit{BigQuery} in this project for a few main reasons:
\begin{itemize}
  \item Expressive syntax without sacrificing testability of code
  \item Keeps class responsibility narrow and well defined
  \item Clean constructor injection to automatically connect to \textit{Google Services} and access the \textit{BigQuery} API
  \item Explicit declaration defines what the class needs and what the class does
\end{itemize}

\section{SciPy}
SciPy is a Python based ecosystem of open-source software geared towards mathematics, science and engineering. In particular, this project utilizes the NumPy package for array manipulation and processing, the SciPy package for the hierarchical clustering, linkage matrix generation and dendrogram presentation and finally the Matplotlib package for plotting and displaying the dendrogram. Each of the utilizations of the packages are broken down further in later sections as they are employed.

\chapter{Algorithms and Methods}
This chapter describes algorithms integral to the key components of \textit{Rally}. The hierarchical clustering of a subreddit and techniques for image classification are described in detail.
\section{Hierarchical Clustering}
When observing an open environment, a powerful metric for how the community is distributed is discovered with clustering. One of the biggest benefits of hierarchical clustering is that you do not need to know the number of clusters in the data set going in to the analysis. It is with hierarchical clustering that within a subreddit, we are able to detect sub-communities. Strategies for hierarchical clustering land within two groups: agglomerative and divisive. Agglomerative is a bottom up approach where each observation starts in its own cluster and pairs are merged as you move up the hierarchy. Divisive is a top down approach where all observations begin in a single cluster and are split recursively down throughout the hierarchy. 
\par
To best understand the hierarchical clustering process, we will begin by showing the end result in what is known as a dendrogram. A clustering of users amongst the subreddit \textit{/r/movies} is shown as a dendrogram in Figure \ref{fig:moviesdend}. The dendrogram is a visualization in the form of a tree that shows the order and distances of merges throughout the hierarchical clustering. It can be understood as snapshots throughout the linkage of observations. On the x axis are labels representing numbers of samples (if in brackets) or specific samples (without brackets). On the y axis are the relative distances (using the 'ward' method described later). Beginning at the bottom of the lines (near the labels), the height of the horizontal joining lines tells us about the distance at which that labelled group merged with another label or cluster. 

\begin{figure}[H]
\includegraphics[width=\textwidth]{movies_dendogram.png}
\caption[Dendrogram of /r/movies]{
A dendrogram representing the hierarchical clustering amongst the subreddit \textit{/r/movies}.}
\label{fig:moviesdend}
\end{figure}

\par
For the example shown in Figure \ref{fig:moviesdend} there are 4265 samples (users) being processed. What is actually being shown is a truncated dendrogram, showing only the last 12 merges. The small black dots along the vertical lines represent joins that happened prior to the final 12. Truncation is an incredibly useful tool when plotting dendrograms. More often than not, we are only interested in the last few merges amongst the samples. The merge that carries the largest vertical distance will be the merge that attaches the most segregated groups. Again with the example in Figure \ref{fig:moviesdend} we see three distinct groups being formed, identified by their green, red and teal colours (left, center and right groups respectively).
\par
Before summarizing the process, here is a concise list of the variables and what they map to:
\begin{itemize}
\item{X: samples (n*m array), or data points or "singleton clusters"}
\item{n: number of samples}
\item{m: number of features}
\item{Z: cluster linkage array}
	\begin{itemize}
	\item{Contains the hierarchical clustering information}
	\end{itemize}
\item{k: number of clusters}
\end{itemize}
\par

\subsection{The Clustering Process}
To begin the clustering, we first gather the necessary data from \textit{Google BigQuery}. The query retrieves the most recent 300 posts for the specified subreddit. A join is then made with the \texttt{link\_id} from the inner query and a \sqlword{UNION ALL} with the comment shard tables over the past 3 months. \textit{BigQuery} does not directly support the \sqlword{UNION ALL} syntax familiar to most sql languages, but instead supports comma separated tables wrapped in a \sqlword{SELECT *}. After joining up the relations, user accounts that were deleted or made by an auto moderator are filtered out. The remaining authors are grouped by \texttt{link\_id} and selected out by the number of times they commented on each link. The query as it is executed in the application can be seen in Figure \ref{fig:clusterquery}. The query processes 9.95GB of data across a total of 4 tables and is completed between 5 and 10 seconds (depending on the subreddit under consideration).

\begin{figure}[H]
\begin{lstlisting}[showstringspaces=false]
SELECT author, link_id, COUNT(link_id) as cnt 
FROM ( 
  SELECT * 
  FROM 
  [fh-bigquery:Reddit_comments.2016_01], 
  [fh-bigquery:Reddit_comments.2015_12], 
  [fh-bigquery:Reddit_comments.2015_11] 
)
WHERE link_id IN ( 
  SELECT posts.name 
  FROM [fh-bigquery:Reddit_posts.full_corpus_201512] AS posts 
  WHERE posts.subreddit = (%subreddits) 
  AND posts.num_comments > 0 
  ORDER BY posts.created_utc DESC LIMIT 300 
) 
AND author != `[deleted]' 
AND author != `AutoModerator' 
GROUP BY author, link_id 
ORDER BY author
\end{lstlisting}
\caption[BigQuery for retrieving clustering data]{
The query executed on \textit{BigQuery} to retrieve all cluster data.}
\label{fig:clusterquery}
\end{figure}

Upon retrieving the data, the X matrix needs to be generated which has n samples and m features. Samples are authors of comments on listings and features are each of the listings. In Figure \ref{fig:responsealgo} is the algorithm for processing the raw \textit{BigQuery} response in to a usable matrix. Because the matrices can become very large in size, we are currently limiting the data gathered by using only the most recent 300 posts. Future work could focus on coming up with a preprocessing technique to predict the anticipated size of response data from \textit{BigQuery} and select an appropriate post number.

\begin{figure}[H]
	\begin{algorithm}[H]
		\SetKwComment{tcc}{// }
		\SetAlgoLined
		\Input{raw BigQuery table response}
		\Output{n * m matrix of users and submissions with comment frequency values}
   	 	\For{each row in response} 
   	 	{
   	 		\tcc{Save the frequency a user commented on a post}
   	 		values\lbrack author\rbrack\lbrack linkid\rbrack = count\;
      		\tcc{Save unique users}
			\If{user has not been seen before}{
				\tcc{Append username to users array}
				users\lbrack\rbrack = user\; 
			}
			\If{link has not been seen before}{
				\tcc{Append link to links array}
				links\lbrack\rbrack = link\; 
			}
    		}
    		\For{each user in users}
    		{
    			\For{each link in links}
    			{
    				\tcc{If a user has commented on a link}
    				\eIf{values\lbrack user\rbrack\ has array key link} {
    					\tcc{Set [user][link] = count}
    					result\lbrack user\rbrack\lbrack link\rbrack = values\lbrack user\rbrack\lbrack link\rbrack\;
    				}{
    					result\lbrack user\rbrack\lbrack link\rbrack = 0;
    				}
    			}
    		}
    		return result\;
    	\end{algorithm}
\caption{Preparing the \textit{BigQuery} response data for clustering}
\label{fig:responsealgo}
\end{figure}

Upon generating the X matrix, the results are dumped out to a json encoded file. The path to the json file is then passed along with a call to execute the Python script.

Generating the linkage matrix \textit{Z} in Python with the help of \textit{SciPy} is straightforward. An \textit{(n-1)} by 4 matrix \textit{Z} is returned. At the \textit{i}-th iteration, clusters with indices \textit{Z[i, 0]} and \textit{Z[i, 1]} are combined to form cluster \textit{n+i}. A cluster with an index less than \textit{n} corresponds to one of the \textit{n} original observations. The distance between clusters \textit{Z[i, 0]} and \textit{Z[i, 1]} is given by \textit{Z[i, 2]}. The fourth value \textit{Z[i, 3]} represents the number of original observations in the newly formed cluster. The algorithm starts with a forest of clusters. When two clusters \textit{s} and \textit{t} from this forest are combined in to a single cluster \textit{u}, \textit{s} and \textit{t} are removed from the forest and \textit{u} is added to the forest. The algorithm is complete when only one cluster remains in the forest and this cluster becomes the root. A distance matrix is maintained at each iteration. 

The \textit{d[i,j]} entry corresponds to the distance between cluster \textit{i} and \textit{j} in the original forest. At each iteration, the algorithm must update the distance matrix to reflect the distance of the newly formed cluster \textit{u} with the remaining clusters in the forest.

There are multiple methods for calculating the distance between newly formed clusters \textit{u} and \textit{v}. We elect to use the ward method. Suppose there are \textit{|u|} original observations \textit{u[0],...,u[|u|-1]} in cluster \textit{u} and \textit{|v|} original objects \textit{v[0],...,v[|v|-1]} in cluster \textit{v}. Recall \textit{s} and \textit{t} are combined to form cluster \textit{u}. Let \textit{v} be any remaining cluster in the forest that is not \textit{u}. Given these definitions for observations and objects, the ward method calculates distance between the newly formed cluster \textit{u} and \textit{v} as follows in equation \ref{warddistance}. Where \textit{u} is the newly joined cluster consisting of clusters \textit{s} and \textit{t}, \textit{v} is an unused cluster in the forest, \textit{T = |v| + |s| + |t|}.

\begin{equation}
\label{warddistance}
\sqrt{\frac{|v| + |s|}{T}d(v,s)^2 + \frac{|v| + |t|}{T}d(v,t)^2 - \frac{|v|}{T}d(s,t)^2}
\end{equation}

The final piece of the puzzle is visualizing the results using a dendrogram as introduced at the beginning of this section in Figure \ref{fig:moviesdend}. The full code used to visualize the linkage matrix is outlined in Figure \ref{fig:matplotlibdend}. As we can see, by simply specifying title, label, turning parameters and p (the number of final merges to show) produces an intuitive dendrogram with clear colour and stage distinction.

\begin{figure}[H]
\begin{lstlisting}
plt.title('Hierarchical Clustering Dendrogram (truncated)')
plt.xlabel('sample index or (cluster size)')
plt.ylabel('distance')
plt.gcf().subplots_adjust(bottom=0.15)
dendrogram(
        Z,
        truncate_mode='lastp',  # show only the last p merged clusters
        p=12,  # show only the last p merged clusters
        leaf_rotation=90.,
        leaf_font_size=12.,
        show_contracted=True,  # to get a distribution impression in truncated branches
)
# plt.show()
plt.savefig('/your/file/location/cluster.png')
\end{lstlisting}
\caption[Drawing the dendrogram using matplotlib]{
Drawing the dendrogram using matplotlib.}
\label{fig:matplotlibdend}
\end{figure}

Summary of the results of a dendrogram:
\begin{itemize}
\item{horizontal lines are cluster merges}
\item{vertical lines tell you which clusters/labels were part of merge forming that new cluster}
\item{heights of the horizontal lines tell you about the distance that needed to be "bridged" to form the new cluster}
\end{itemize}
It is the distance jumps and gaps in the dendrogram that are of value when interpreting the data. When the jump is large, it indicates that two groups are being merged together that maybe should not be merged. In other words, we have identified two potentially unique groups that form independent clusters.

\section{Image Classification}
Image recognition was employed to effectively automate and scale the labelling of media content submitted to \textit{Reddit}.
\par
Human brains make vision seem very trivial, it doesn't take much effort for humans to distinguish between a jar of alphagetti and a wasps nest (a seemingly very random example but proved to actually be a difficult task). But these are very hard problems to solve with a computer, they only seem easy because our brains are incredibly good and understanding visual queues. 
\par
In the last few years the field of machine learning has made tremendous progress on addressing these difficult problems. In particular, deep convolutional neural networks can achieve reasonable performance on hard visual recognition tasks. Often matching or exceeding human performance in some domains \citep{TensorFlow}.
\par
To classify images with labels, a first attempt was made using \textit{Google TensorFlow}, the recently open sourced machine learning toolkit by Google. In particular, we focused on implementing and leveraging the power of Inception-V3 \citep{DBLP:journals/corr/SzegedyVISW15} - the newest model for identifying higher level features into classes.
\par
\textit{TensorFlow} is a very complex API for programmers to use either CPU, GPU or in some cases both (using \textit{CUDA}) devices. The barrier to entry is quite high but upon learning the flow of data and architecture of the infrastructure is a very powerful tool. We will not go in to detail on the implementation as it is not relevant to the underlying use.
\par
Upon implementing the image recognition class using \textit{TensorFlow}, an API was built that allowed for convenient calls to classify images sent along as POST data. This system was optimal as it allowed for independent testing and debugging. The major downfall was the lack of speed with the implementation for analysing the image. To reduce the computation time, \textit{CUDA} was used and the algorithm was altered to run in parallel on a GPU. The main struggle with this implementation was working with the \textit{TensorFlow} API on a GPU that did not support the latest version of \textit{CUDA}, which was the only version \textit{TensorFlow} is currently (as of April 2016) targeting. 
\par
When the GPU version of the image classification was finalized and tested, computation time was cut in half, but it still took anywhere between 2 and 6 seconds to analyse a single image. The results of the classification were also dissatisfying as it only achieved an accuracy of roughly 60\%. It was difficult not to give in to the sunk cost of sticking with an approach that was built over the course of a month however, as discussed in the implementation section it was undoubtedly the correct choice to abandon \textit{TensorFlow}.

\chapter{Implementation}
An in depth overview of the technical implementation of an API wrapper, \textit{Rally} and \textit{RallySearch}.
\section{phpRaw}
The \textit{Reddit} API has several endpoints. It is through these endpoints where a client can retrieve posts specific to a subreddit, post a comment, moderate their account and all other actions that are normally available through the consumable web interface. For a single use or specific focus, calling the endpoints explicitly with \textit{cURL} (or another client-side URL transfer) works fine but this strategy quickly fails as needs grow. Due to the wide array of endpoint calls utilized, it was necessary to develop an API wrapper that allows convenient calls to the API. Such a wrapper already existed for Python, Java, C and a few other languages but not PHP.
\par
An open source wrapper was discovered on \textit{GitHub} but was no longer maintained, was not written to comply with the latest API security requirements (OAuth2) and was missing nearly half of the endpoints. Building on the work done on this API wrapper, a successful implementation was built and is what \textit{Rally} utilizes and depends on for direct \textit{Reddit} data access. The \textit{GitHub} repository from the point at which it was forked and built on is linked in the appendix.
\par 
Listed below are functions from \textit{phpRaw} to give a feel for the wrapper.
% Get the user submitted data
\par
Get the user submitted data.
\begin{lstlisting}
$phpRaw->getUserSubmitted($user, $limit = 25, $after = null);
\end{lstlisting}

% Get the top 10 hottest listings for a specified subreddit
Get the top 10 hottest listings for the specified subreddit, `funny'.
\begin{lstlisting}
$phpRaw->getHot(`funny', 10);
\end{lstlisting}
\par
\textit{phpRaw} was modified to serve as a standalone vendor service brought in through Laravel's default dependency manager \textit{Composer}. By extracting the wrapper to a separate module, updating and maintaining the endpoints is simple as they are changed over time. Using the power of composer and package dependencies, by including the declaration as outlined in Figure \ref{fig:composer}, whenever \textit{Composer} is updated it automatically updates to the latest version of \textit{phpRaw}.

% Composer.json for phpRaw
\begin{figure}[H]
\begin{lstlisting}
...
"repositories": [
	{
		"name": "kevin/phpRAW",
		"type": "vcs",
		"url": "https://github.com/kevineger/phpRAW"
	}
],
...
\end{lstlisting}
\caption[Requiring phpRaw as a dependency in composer.]{
Requiring \textit{phpRaw} as a dependency for \textit{Rally} in composer.}
\label{fig:composer}
\end{figure}

\section{Rally}
Rally combines user statistics, big data, subreddit analysis and RallySearch into one convenient location.
\subsection{User Statistics}
On \textit{Reddit}, users have the ability to view their recent activity (comments, submissions and saved content) and link/comment karma scores. \textit{Reddit} serves this information in a similar fashion to how they display submissions on their site. This technique is effective for listing out a history of comments and submissions but it proves ineffective for quickly interpreting account details. To alleviate this lack of accessibility, upon entering a user name on the "User Stats" page, users can quickly see the following information on a user:
\begin{itemize}
\item{User Card}
	\begin{itemize}
	\item{Username}
	\item{Unique ID}
	\item{How long they have been a user for}
	\item{Gold and mod status}
	\end{itemize}
\item{Activity over time (Submissions vs. Time of Day)}
\item{Submission Data}
	\begin{itemize}
	\item{Karma}
	\item{Top submission (with link)}
	\item{Worst submission (with link)}
	\item{Most recent submission (with link)}
	\item{Average karma}
	\end{itemize}
\item{Comment Data}
	\begin{itemize}
	\item{Karma}
	\item{Top comment (with link)}
	\item{Worst comment (with link)}
	\item{Total comments (with link)}
	\item{Average karma}
	\end{itemize}
\item{Itemized and labelled list of subreddits posted to with badge counts for frequency and highlighting for top}
\end{itemize}

The user statistics section gathers information exclusively from the API wrapper, \textit{phpRaw}. Calls to the wrapper are made through a repository layer, as described in the technical stack section under the MVC subsection. It is in surrounding the \textit{phpRaw} calls with a repository layer we can save on API calls and thus reduce the time it takes to gather the necessary information through method chaining. An example call from the controller to the repository which in turn calls \textit{phpRaw} is seen in Figure \ref{fig:repotophpraw}. When \texttt{getUserSubmitted(\$user)} is executed, a large response object consisting of most the necessary information for the entire page is returned. By storing this information on the object and making the method chainable (returns an instance of the object) we can have easy and most importantly expressive syntax in calling the approriate information, for example:\\ \texttt{\$this->redditor->getUserSubmitted(\$user)->getTopUpVotes();}.
\begin{figure}[H]
\begin{center}
\begin{lstlisting}
$subreddits = $this->redditor->getUserSubmitted($user)->getSubredditsList();
\end{lstlisting}
\end{center}
\caption[Example of call to phpRaw through controller via repository]{
An example of a call to \textit{phpRaw} through the controller via a repository.}
\label{fig:repotophpraw}
\end{figure}

To demonstrate the importance of wrapping the API with \textit{phpRaw} and the ease of calling it, the php code for generating the vector of active hours for the "Activity Chart" can be seen in Figure \ref{fig:activehours}.

\begin{figure}[H]
\begin{center}
\begin{lstlisting}
public function activeHours()
{
	$hours = array_fill(0,24,0);
	foreach ($this->getSubmissions() as $submission)
	{
		$time = Carbon::createFromTimestampUTC($submission->data->created_utc);
		$hour = $time->hour;
		$hours[$hour]++;
	}

	return $hours;
}
\end{lstlisting}
\end{center}
\caption[Code to generate vector of active hours]{
The code to generate the vector of active user hours.}
\label{fig:activehours}
\end{figure}

\subsection{Subreddit Clustering}
The information on what hierarchical clustering is and how it was implemented are discussed in detail in Section 4.1. This section will discuss the results from the utilization of clustering and how they are presented to the user.
\par
Upon landing on the "Subreddit" page and entering the desired subreddit, two main pieces of information are presented: "Sub Info" - basic information card about the subreddit and "Clustering" - the dendrogram.
\par
The "Sub Info" card contains the subreddit name, unique ID, subreddit logo (if applicable), motto, description, current number of users and subscribers.
\par
The "Clustering" section displays the dendrogram as previously seen in Figure \ref{fig:moviesdend}. Prior to implementing the dendrogram, it was anticipated that sub-communities amongst subreddits would identify and be focused around a subset of conversation topics/themes amongst posts, this was not the case. Upon tracing through some of the dendrograms and examining the specific groups and linkages, it was discovered that "types" of users were in fact the attribute represented by the clustering. In nearly all cases, the largest group clustered together were that of sparse users who comment only a handful of times and infrequently at that. In most other cases there are two well defined groups: those that comment on nearly all of the top submissions and those that comment on just a couple. The results amongst various subreddits have clear distinctions and defining features but tend to follow the same patterns.

\subsection{Big Data}
The Big Data page of \textit{Rally} is intended to harness and demonstrate the power of \textit{Big Query}. From tables spanning sizes of megabytes to tens and twenties of gigabytes, \textit{Big Query} delivers the fastest out of the box relational cloud database.
\par
\includegraphics[width=\textwidth]{bigquery.png}
The first big data snippet on the page is the "Activity Over Time" chart. Users can easily enter the desired subreddits for analysis in the selection box and the graph is redrawn and served to the user in a seamless, AJAX request. As expected, most subreddits take a dip in activity during the night-time (North American timezones). What is interesting is paring subreddits together that are very similar, for example /r/Programming and /r/ProgrammerHumor. Subreddits with an almost directly equal subscriber list follow a nearly identical activity over time, just with more or less amplitude.
\par
Because \textit{Reddit} is a "reward-based" service (you earn karma on submissions and comments), users often inquire when the best time to post a submission is. This question can be accurately resolved by leveraging the speed with which \textit{Big Query} can read high cardinality tables. By grabbing the highest subscribed subreddits, the results are generated with the query listed in Figure \ref{fig:besttime}.
\begin{figure}[H]
\begin{center}
\begin{lstlisting}
SELECT GROUP_CONCAT(STRING(sub_hour)) as hours, subreddit, SUM(num_gte_3000) total 
FROM ( 
	SELECT HOUR(SEC_TO_TIMESTAMP(created - 60*60*5)) as sub_hour, SUM(score >= 3000) as num_gte_3000, subreddit, RANK() 
	OVER(PARTITION BY subreddit ORDER BY num_gte_3000 DESC) rank, 
	FROM [fh-bigquery:Reddit_posts.full_corpus_201509] 
	WHERE YEAR(SEC_TO_TIMESTAMP(created))=2015 
	GROUP BY sub_hour, subreddit 
	HAVING num_gte_3000 > 100 
) 
WHERE rank<=3 
GROUP BY subreddit 
ORDER BY total DESC
\end{lstlisting}
\end{center}
\caption[Query for getting the best time to post on various subreddits]{
Query for getting the best time to post on various subreddits.}
\label{fig:besttime}
\end{figure}

Similarly, three other tables exist on the page answering common questions amongst the community. The source code for the queries can be found in \texttt{rally/config/constants.php}. Their titles with descriptions are listed here:
\begin{itemize}
\item{Most popular comments on \textit{Reddit}}
	\begin{itemize}
	\item{Rank}
	\item{Count frequency}
	\item{Comment body}
	\item{Average score}
	\item{Count of subreddits comment exists on}
	\item{Count of authors using this comment}
	\item{An example use case of the comment (link to \textit{Reddit})}
	\end{itemize}
	
\item{Difference in Cohorts (Account Creation Date)}
	\begin{itemize}
	\item{Year account was created}
	\item{Number of users}
	\item{Average number of comments}
	\item{Number of users still presently active}
	\item{Sum of their score}
	\item{Number of gilded users}
	\item{Average body length of comments}
	\end{itemize}
	
\item{Number of comments by day of the week}
	\begin{itemize}
	\item{Day of the week}
	\item{Number of comments}
	\end{itemize}
\end{itemize}

To demonstrate the breadth of possibility in analysing community-based services like \textit{Reddit}, a query and visualisation of the U.S. election candidate mentions was generated. A screenshot of the generated graph can be seen in Figure \ref{fig:uselectionsscreen} and the query utilized in Figure \ref{fig:ustopcand}.

\begin{figure}[H]
\includegraphics[width=\textwidth]{uselectionscreen.png}
\caption[U.S. elections candidate mention frequency]{
U.S. elections candidate mention frequency screenshot.}
\label{fig:uselectionsscreen}
\end{figure}

\begin{figure}[H]
\begin{center}
\begin{lstlisting}[showstringspaces=false]
SELECT DATE(USEC_TO_TIMESTAMP(UTC_USEC_TO_WEEK(created_utc*1000*1000,1))) week, SUM(body CONTAINS "Bernie Sanders") BernieSanders,SUM(body CONTAINS "Hillary Clinton") HillaryClinton,SUM(body CONTAINS "Donald Trump") DonaldTrump,SUM(body CONTAINS "Ted Cruz") TedCruz 
FROM TABLE_QUERY([fh-bigquery:Reddit_comments], 'REGEXP_MATCH(table_id, "201._..$")') 
GROUP BY 1 
ORDER BY 1
\end{lstlisting}
\end{center}
\caption[Query for getting the frequency of U.S. candidate mentions]{
Query for getting the frequency of top U.S. candidate mentions.}
\label{fig:ustopcand}
\end{figure}

\section{RallySearch}
\textit{RallySearch} is an alternative way of consuming \textit{Reddit}. Users of \textit{Reddit} are very limited by the existing search functionality. Though there exists a search bar, it is incredibly ineffective and displays close to no results as intended. \textit{Reddit} developers have expressed interest in improving the functionality but have not disclosed any immediate plans to do so. This portion of the thesis aims to offer an alternative to browsing \textit{Reddit} by its content in a visually pleasing and simple fashion. The name of this standalone service has been dubbed \textit{RallySearch} and as discussed later has been launched and received by the \textit{Reddit} community.

\includegraphics[width=\textwidth]{rallysearch_screenshot.png}

\subsection{Overview}
The overall goal of \textit{RallySearch} is to provide users with the opportunity to search \textit{Reddit} by its content - the physically linked images, videos, gifs and articles. \textit{Reddit}'s existing flow for browsing content does not give users the ability to search site-wide for all posts consisting of a specific object. For example, if a user wanted to view all posts pertaining to dogs and cats, they would have to manually search through all the subreddits. This may or may not contain the desired content and the only option they would have left is to perform a \textit{Google} search which is even more inefficient and guarantees no degree of accuracy. Using \textit{RallySearch}, labels can easily be specified using the search bar at the top and the page instantly loads all existing classified posts.

When a user first lands on the site, they are presented a page already populated with cards which represent posts. A card consists of the post preview image, title, labels for the image (from image recognition) as well as a few other details and options for navigation discussed later in the UI section.

In summary, top media posts on \textit{Reddit} are sent through \textit{Google Cloud Vision} (Image recognition API) and results are cached. Upon giving each image (or preview in the case of videos and gifs) labels, users can easily view similar content by selecting the desired tag(s). 

\subsection{User Interface}
A good user interface (UI) can make or break the success of a website. Too complicated or busy and the site could be left collecting dust on the internet shelves. With \textit{Rally}, the UI balances a clean look with the necessary interactivity coupled with ease of use and simple navigation. The standalone portion of the site \textit{RallySearch} will be surveyed and the design choices that went in to designing it as it is slightly more polished than Rally but represents all the same key features.
\subsubsection{Semantic UI}
\textit{Semantic UI} is a framework designed for site theming. Key features include predifined CSS for elements, variable tuning, inheritance and responsiveness. Semantic is free, open sourced and MIT licensed. It allows developers to ``build beautiful websites fast'', with concise HTML, intuitive javascript and simplified expressive CSS class naming.
\subsubsection{RallySearch}
Near all the interaction on the site takes place on the main index page of the site. Here, users can browse the content and filter by tags if they so wish to.

\includegraphics[width=\textwidth]{rallyindexpage.png}

When a user lands on the index page, the most recent annotations are displayed and are paginated in groups of 20. Splitting the pages is necessary as to avoid loading thousands of models from the database and to avoid screen clutter and ``jankiness'' (a web design term to describe when a screen stutters while loading dynamic content added with \textit{JQuery}).
\par
Each of the annotations are divided up in to cards. A card displays content in a manner similar to a playing card. There is the preview image which is loaded externally from \textit{Reddit} to reduce server load and increase performance, the listing title which is trimmed with ellipsis, the subreddit the image was posted to, all labels given to the annotation and the card functionality.
\begin{figure}[H]
\begin{multicols}{2}
    \includegraphics[width=\linewidth]{card.png}\par 
    \includegraphics[width=\linewidth]{card_hover.png}\par 
\end{multicols}
\caption{A single card without and with hover.}
\end{figure}
A single card has a few aspects of functionality associated with it. First and most important is the ability to follow the link through to \textit{Reddit}. RallySearch was designed to simply be a more convenient gateway to \textit{Reddit} content, thus by hovering and clicking "View on \textit{Reddit}" or directly clicking the submission title, the user is redirected to the submission.
\par
If a user wishes to view a larger version of the image, the full title and label list of a card, they simply have to select the "Expand" button in the bottom left. Modals display content in a way that temporarily blocks interactions with the main view of the site. This is really effective for ensuring the user is focusing on the desired content and abstracted from the other "distractions" on the site.
\begin{figure}[H]
\includegraphics[width=\textwidth]{modal.png}
\caption{A card's modal.}
\end{figure}
\par
Since the purpose of the site is to browse \textit{Reddit} by labels (content), it makes sense there are a few approaches to do so. First and most clear is the ability to type into the dropdown. As the user enters text, their search is refined from the list of pre-existing labels. There is no limit to the number of labels a user can enter however they must select from the pre-existing labels as all results are loaded from already classified annotations. Users also have the option to select the "Similar" button on a card which instantly loads the corresponding tags into the dropdown. The final way a user can refine their search is by directly clicking on a label in a card. If a user wishes to restart their search, they are free to click the clear button located to the right of the dropdown.
\begin{figure}[H]
\begin{multicols}{3}
    \includegraphics[width=\linewidth]{dropdown_search.png}\par
    \includegraphics[width=\linewidth]{similar.png}\par
    \includegraphics[width=\linewidth]{labelclick.png}\par
\end{multicols}
\caption{A single card without and with hover.}
\end{figure}

To make browsing the site as fluid as possible, AJAX get requests are made to the server whenever new content is needed. Whether the user is advancing pages, adding labels or clearing them, the site fetches only the new necessary information and swaps it out with the current. This experience is preferable as it removes the ``jankiness'' with traditional websites when switching between pages. A single-page application (SPA) is preferable as the fluid experience that is similar to a desktop application does not interrupt the user's actions on the site associated with switching between pages. The dynamically loaded content is also optimized to be as slim as possible, loading quicker than any possible full page reload. 
\par
\textit{RallySearch} is equipped with a very slim and stylish menu bar. The bar is hidden by default but can be activated by click the menu tab always located in the top left of the screen. The tab when not active is an icon of three lines which has become the universal standard for representing context menus that all users are used to interacting with. When the user hovers the icon, it dynamically expands reading "Menu". If the user then clicks on the tab, the menu rolls in from the left pushing the site content slightly the the right in a fluid and smooth fashion. The main content is then dimmed similar to when the modal is displayed as to focus the user on the action they are undergoing.
\begin{figure}[H]
\begin{multicols}{2}
    \includegraphics[width=\linewidth]{menu_closed.png}\par 
    \includegraphics[width=\linewidth]{menu_open.png}\par 
\end{multicols}
\caption{The site menu closed and open.}
\end{figure}

\subsection{Technical Overview}
\textit{RallySearch} is a slim and highly maintainable web project. It uses two external services, phpRaw (\textit{Reddit} API wrapper created for this project discussed previously) and \textit{Google Cloud Vision}. The technical workings of \textit{RallySearch} can be divided in to two main domains: building the service and consuming the service.
\subsubsection{Building the Service}
The Google Cloud Vision API enables developers to understand the content of an image by powerful machine learning models in an easy to use REST API. Images are quickly classified into thousands of categories. Individual objects and faces can be detected within an image. The broad set of objects in images are categorized and help improve the Vision API over time as new concepts are introduced and accuracy is improved. The key feature of the API that \textit{RallySearch} uses is label detection. With it, broad sets of categories are detected within an image ranging from dog breeds to wedding dresses.
\par
\textit{RallySearch} is written with full modularity in mind. This is important because APIs are always subject to change over time and it incorporates two (\textit{phpRaw} and \textit{Cloud Vision}). To accomplish this, a Laravel Job is written which runs on a schedule. The job is executed hourly and at a high level, does the following:
\begin{itemize}
\item{Creates a new collection}
	\begin{itemize}
	\item{\textit{Laravel} iterable object that wraps PHP array}
	\end{itemize}
\item{Retrieves the \textit{hottest} 100 posts on \textit{Reddit}}
\item{Retrieves the \textit{top} 100 posts on \textit{Reddit}}
	\begin{itemize}
	\item{Daily}
	\item{Weekly}
	\item{Monthly}
	\item{Yearly}
	\item{All time}	
	\end{itemize}
\item{Creates Annotation objects for each of the new listings}
	\begin{itemize}
	\item{Checks against database if record already exists}
	\item{Stores unique \textit{Reddit} id, post url, image url (preview), post title, subreddit post was submitted to}
	\end{itemize}
\item{Labels each of the Annotations}
	\begin{itemize}
	\item{Sets labelling parameters (feature type: label and max results: 10}
	\item{Builds batch annotation image request object}
	\item{Downloads the image to be analyzed and encodes in base 64 to be sent with the request(s)}
	\item{Sends request and saves response}
	\item{Saves labels for each image, creating new Label objects when needed and reusing existing ones when applicable (An Annotation has many Labels and a Label belongs to many Annotations}
	\end{itemize}
\end{itemize}

The code for this process is slightly too long to include in the thesis but is easily accessible in the repository listed in the bibliography.

\subsubsection{Consuming the Service}
The main motivation for creating \textit{RallySearch} was to offer a convenient and easy to use way of browsing \textit{Reddit}. Keying in on this motivation it was crucial users were provided with an interface that was easy to adopt and was non intrusive. More on the details of the interface can be found in the UI section, here we will go over a typical users use-case of the site.
\par
The service itself only exists on one page: the content page. Users can browse all labelled posts by scrolling through cards and advancing through pages. For a more refined experience a user can enter a single or multiple tags in the search bar at the top. The bar allows users to only select from a predefined list (labels which exist in the database). Upon clicking or selecting a label(s), the site loads up the appropriate content.
\par
Users are free to navigate their desired content, expand the preview for a larger view of the media and select similar tags by manually selecting those on a post or by using the "Similar" button.

\subsubsection{Deployment}
\textit{RallySearch} is deployed on \textit{DigitalOcean}, a cloud computing hosting company. The web app runs on their cheapest tier server (Ubuntu 14.04 with 512MB of memory, 20GB disk space) and performs beyond the needed level. Due to the efficiency and optimization in the web app, it is estimated that this server configuration could serve more than twice the highest reached concurrent users (approximately 50) before suffering any performance costs. 
\par
\textit{RallySearch} has continuous deployment through a third party service called \textit{Codeship}. Each time changes are pushed to the master branch of the repository, \textit{Codeship} is triggered by a GIT hook which brings in the changes, updates the dependencies through composer, dumps all autoloaded and caches and seamlessly brings the server up to date as expected. \textit{Codeship} first brings a virtual server to life with the newest code then copies the changes over to the production code ensuring a near-zero down time. \textit{RallySearch} ensures only code passing the specified automated tests is deployed to production and notifies immediately if any push does not succeed.

\subsection{Public Reception}
Since \textit{RallySearch} is a service for the community and arguably \textit{built by the community} (all the content is theirs), there was a strong desire to push the service live and generate feedback.
\par
\textit{RallySearch} was made publicly available and posted to a few subreddits intended for developers and enthusiasts to give feedback and test the service. Subreddits the site was posted to include: \textit{/r/Frontend} - a subreddit for front end web developers who want to move the web forward or want to learn how, \textit{/r/UsefulWebsites} - a compilation of useful websites and \textit{/r/design\_critiques} - a subreddit to receive critiques on a design. Over the course of just one day, submissions on \textit{/r/Frontend} and \textit{/r/UsefulWebsites} made it to the top submission of both subreddits and are amongst some of the top posts there over the last year. Though these subreddits are not heavily trafficked, it was a noteworthy accomplishment due to the highly opinionated, dogmatic and domineering nature of the respectful subreddits.
\par
Rather than listing all of the comments, below is a list of a few of the top comments (most upvoted) and most helpful comments that were reflected with changes and improvements.
\begin{itemize}
\item{``To be honest the accuracy of the tags blows my mind'' - /u/r\_park}
\item{``If I'm scrolled down the page, selecting the "similar" button appears to do nothing if it doesn't happen to populate more content. It also seems unintuitive that it adds more tags to my existing tags. This interaction is overall a good idea but a little unclear in terms of what it actually does.'' - /u/wayspurrchen}
\item{``The content hierarchy for the videos doesn't make a lot of sense/isn't very useful: http://i.imgur.com/RWdyvDL.png As a casual user, I'm unlikely to ever care about the permalink, and I'm most likely to care about the title. The title should also be clickable to take me to \textit{Reddit}. I swapped things around a bit: http://i.imgur.com/1y451Q7.png You could even put something else there like the score for that subreddit, number of comments, etc.'' - /u/wayspurrchen}
\item{``Overall this is a really killer app. I can't wait to see you polish it! :)'' - /u/wayspurrchen}
\item{``You are my new hero.'' - /u/ChaosElephant}
\item{``This is great - looks nice as well. Awesome job!
My only critique at the moment would be to have the full title reveal in some sort of tooltip or something, even just as a title attribute tooltip. At first I didn't notice the expand, or you may just think it expands the photo. It would be nice to see the full title without clicking.'' - /u/hidanielle}
\end{itemize}

It was great to get the service out there for a non-subjective set of eyes to critique, advise and shape it. Beyond having other \textit{Reddit} users test the site, friends and family were also good help in offering advice and feedback from a new user (unfamiliar with \textit{Reddit}) perspective. Though proper and documented user testing was not conducted, key takeaways with this group include:
\begin{itemize}
\item{Hooking up links that were expected to direct somewhere but did not initially.}
\item{Reorganizing the card structure}
\item{Altering the navigation menu for a more responsive feel}
\end{itemize}

\subsection{Analytics}
\textit{Google Analytics} were implemented and utilized upon launch. This allowed for the understanding of consumer behaviour, gathering insights, tracking usage numbers and analysing performance of the site. Data tidbits from the analytics are outlined here:
\begin{itemize}
\item{736 unique visitors in the first 24 hours}
\item{0:56 seconds average session duration}
\end{itemize}
\begin{table}[H]
\begin{center}
\csvautotabular{data/osandbrowser.csv}
\caption{Operating system and browser of users}\label{tab:osandbrowser}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
\csvautotabular{data/country.csv}
\caption{Countries of users accessing the site}\label{tab:country}
\end{center}
\end{table}

\subsection{Open Source}
\textit{RallySearch} is fully open source. The source code is available publicly on GitHub. There has already been a bit of interest with members of the community and the repository has had three stars from developers. The software is MIT licensed. 

\subsection{Moving Forward}
The initial launch of \textit{RallySearch} has shown the potential to move forward as an individual project. Though there is no intention to have users strictly use the service and abandon browsing \textit{Reddit} directly, the general consensus has been that it is convenient to relax and browse specific content tailored to a user's interests. Before launching the project and releasing it to the general public by advertising on a higher traffic subreddit, a few improvements and changes need to be made:
\begin{itemize}
\item{Automated database backups}
\item{Remove the small but noticeable ``jankiness'' associated with the AJAX page reloading}
\item{Formal user testing}
\item{Build tests}
\item{Update from MySQL to a higher performing database as cardinality increases}
\end{itemize}

\newpage %newpage needed otherwise pagestyle applied to previous chapter. Does not actually create a new page
\pagestyle{fancy}\chead{Bibliography}\rhead{}\cfoot{}\rfoot{\thepage}
\bibliographystyle{ubco}




\chapter{Conclusions}
\textit{Reddit} is forever evolving. The content that is posted, the way the community interacts and the technology itself is always subject to change. Most fields developing tools fear change, as it requires adapting legacy systems or techniques. This project was implemented with that in mind, right from the beginning. All techniques for gathering data, processing and visualizing were built to scale and adapt to change. Though it is true that down the line if a big modification were to happen (ie: the API no longer serves integral data), certain pieces of the software would have to be rewritten. But by and large, the service is solid and built to last.
\par
Having said that, this thesis and project is just the \textit{tip of the iceberg}. The fun part about data analysis is there's no such thing as ``done''. As new techniques and inspirations arise there is always more to add and infer. Though sometimes an analyst's job is to perform a one-off implementation to retrieve a result, the most fun and complex of problems are those that when solved bring about new questions. This style of analysis is greatly fostered in the \textit{Reddit} space.
\par
\textit{Rally} has served as a great proof of concept. Combining multiple 3rd party services can be a nightmare and often requires several rewrites. Having completed a first implementation of the service, it brings great satisfaction claiming that an innovative resource for observing \textit{Reddit} inferences has sprung.
\par
There is always room for improvement, both technically and conceptually. Given the current design of \textit{Rally}'s big-data, subreddit and user statistics pages, releasing the site for general public use would require some optimization and refactoring (especially for caching pre-fetched results). \textit{RallySearch} is a web application recently released in to the wild of the internet and will undoubtedly require bug fixes as they come up and new features as they are requested.
\par
More information and current development on \textit{RallySearch} can be found on the \textit{GitHub} repository (https://github.com/kevineger/rallysearch). Though there is currently no intention to build out \textit{Rally} beyond what it has become, the repository will remain uploaded for anyone who wishes to browse for inspiration or potentially build from.

\bibliography{thesis_references}%name of your .bib fil%e

\end{document}
\endinput
